Loading dataset...
Categorical columns: ['ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type', 'apache_3j_bodysystem', 'apache_2_bodysystem']
Imbalance ratio: 10.59

Training XGBoost...
D:\FINAL_MAJOR\backend\my_env\Lib\site-packages\xgboost\training.py:200: UserWarning: [21:51:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:782:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Accuracy: 0.8762
Precision: 0.3831
Recall: 0.7119
F1-Score: 0.4981
ROC-AUC: 0.9017

Classification Report:
              precision    recall  f1-score   support

           0       0.97      0.89      0.93     16760
           1       0.38      0.71      0.50      1583

    accuracy                           0.88     18343
   macro avg       0.68      0.80      0.71     18343
weighted avg       0.92      0.88      0.89     18343


Training LightGBM...
[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
[LightGBM] [Info] Number of positive: 6332, number of negative: 67038
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025919 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 25270
[LightGBM] [Info] Number of data points in the train set: 73370, number of used features: 233    
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.086302 -> initscore=-2.359643
[LightGBM] [Info] Start training from score -2.359643
Accuracy: 0.8705
Precision: 0.3725
Recall: 0.7315
F1-Score: 0.4936
ROC-AUC: 0.9042

Classification Report:
              precision    recall  f1-score   support

           0       0.97      0.88      0.93     16760
           1       0.37      0.73      0.49      1583

    accuracy                           0.87     18343
   macro avg       0.67      0.81      0.71     18343
weighted avg       0.92      0.87      0.89     18343


Training CatBoost...
Accuracy: 0.8442
Precision: 0.3307
Recall: 0.7865
F1-Score: 0.4656
ROC-AUC: 0.9074

Classification Report:
              precision    recall  f1-score   support

           0       0.98      0.85      0.91     16760
           1       0.33      0.79      0.47      1583

    accuracy                           0.84     18343
   macro avg       0.65      0.82      0.69     18343
weighted avg       0.92      0.84      0.87     18343


Final Results Summary:
          Accuracy  Precision    Recall  F1-Score   ROC-AUC
XGBoost   0.876193   0.383073  0.711939  0.498122  0.901686
LightGBM  0.870468   0.372467  0.731522  0.493606  0.904163
CatBoost  0.844191   0.330677  0.786481  0.465595  0.907409